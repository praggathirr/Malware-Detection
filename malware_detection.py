# -*- coding: utf-8 -*-
"""Malware Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167rctcsvrXGm1f0il4FJcrkDRe1HelvR
"""

!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import torch
from os import listdir
from sklearn import model_selection
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from transformers import BertForSequenceClassification
from transformers import BertTokenizer
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import OneHotEncoder
import numpy as np
# %matplotlib inline 
import matplotlib.pyplot as plt
import seaborn as sns

"""#Read Text Files"""

folder_path = "/content/drive/MyDrive/Malicia (Big 3 - Opcodes)"

ds = [[],[],[]]
def load_file(filename):
  file = open(filename, 'r')
  text = file.read()
  text = text.split('\n')
  text = [s.strip() for s in text]
  file.close()
  return text
i = 0
families = listdir(folder_path)
for family in families[1:]:
  family_path = folder_path + '/' + family
  for file in listdir(family_path):
    file_path = family_path + '/' + file
    ds[i].append(load_file(file_path))
  i += 1
print(len(ds[0]))
print(len(ds[1]))
print(len(ds[2]))

"""# Create DF"""

opcodes = {}

for family in ds:
  for file in family:
    for opcode in file:
      if opcode in opcodes:
        opcodes[opcode] += 1
      else:
        opcodes[opcode] = 1
print(opcodes)

import pandas as pd
opcodes_df = pd.DataFrame.from_dict(opcodes.items())
opcodes_df.rename(columns={0:'opcode',1:'count'}, inplace=True)
opcodes_df.astype({'count':'int64'})
opcodes_df.sort_values(by='count',inplace=True, ascending=False)

print(opcodes_df)

test = []
keys = opcodes.keys()
# for family in ds:
#   for file in family:
for key in keys:
  test.append(ds[0][0].count(key))

final_df = pd.DataFrame([test], columns = keys)

for family in ds:
  for file in family:
    diction = {key: 0 for key in keys}
    for opcode in file:
      diction[opcode] += 1
    final_df = final_df.append(diction, ignore_index=True)

final_df

least_common = opcodes_df[opcodes_df['count'] < 100000]
print(sum(least_common['count'])/sum(opcodes_df['count']))
print(least_common)

targets = [0 for i in range(len(ds[0]))]
winwebsec_df = pd.DataFrame(targets)
winwebsec_df.rename(columns={0:'target'}, inplace=True)
winwebsec_df['files'] = [ ' '.join(i) for i in ds[0]]

targets = [1 for i in range(len(ds[1]))]
zbot_df = pd.DataFrame(targets)
zbot_df.rename(columns={0:'target'}, inplace=True)
zbot_df['files'] = [ ' '.join(i) for i in ds[1]]

targets = [2 for i in range(len(ds[2]))]
za_df = pd.DataFrame(targets)
za_df.rename(columns={0:'target'}, inplace=True)
za_df['files'] = [ ' '.join(i) for i in ds[2]]

raw_df = winwebsec_df.append(zbot_df)
raw_df = raw_df.append(za_df)
raw_df.reset_index(inplace=True)
raw_df

df = final_df.iloc[1: , :]
df.reset_index(drop= True, inplace=True)
prep_df = df.div(df.sum(axis=1), axis=0)
prep_df.drop(columns=least_common['opcode'], inplace=True)
prep_df['other'] = 1 - prep_df.sum(axis=1)
prep_df['target'] = raw_df.target
prep_df

"""#Data Visualization"""

wvis_df = prep_df[prep_df['target'] == 0]
wvis_df.drop(columns=['target'], inplace=True)
data = wvis_df.mean()
data.plot(kind='bar', title='Winwebsec Average Opcode %')

zvis_df = prep_df[prep_df['target'] == 1]
zvis_df.drop(columns=['target'], inplace=True)
data = zvis_df.mean()
data.plot(kind='bar', title='Zbot Average Opcode %')

zavis_df = prep_df[prep_df['target'] == 2]
zavis_df.drop(columns=['target'], inplace=True)
data = zavis_df.mean()
data.plot(kind='bar', title='ZeroAccess Average Opcode %')

"""#KNN"""

param_grid = {'n_neighbors':[1, 2, 3, 4, 5, 6, 7, 8, 10], 'weights':('uniform', 'distance')}
knn = KNeighborsClassifier()
gs = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', return_train_score=True)

"""#Winwebsec vs Zbot"""

df1 = prep_df[prep_df['target'] <= 1]
x_train, x_test, y_train, y_test = model_selection.train_test_split(df1.drop(columns=['target']),df1['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("KNN Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#Winwebsec vs Zeroaccess"""

df2 = prep_df[prep_df['target'].isin([0, 2])]
x_train, x_test, y_train, y_test = model_selection.train_test_split(df2.drop(columns=['target']),df2['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("KNN Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#Zbot vs. Zeroaccess"""

df3 = prep_df[prep_df['target'] > 0]
x_train, x_test, y_train, y_test = model_selection.train_test_split(df3.drop(columns=['target']),df3['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("KNN Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#Multi-Class"""

x_train, x_test, y_train, y_test = model_selection.train_test_split(prep_df.drop(columns=['target']),prep_df['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("KNN Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#RandomForest"""

param_grid = param_grid = {
    'bootstrap': [True, False],
    'min_samples_leaf': [1, 2, 3, 4],
    'min_samples_split': [3, 5, 8],
    'n_estimators': [10, 15, 20, 25, 50]
}
rfc = RandomForestClassifier(random_state=42)
gs = GridSearchCV(rfc, param_grid, cv=5, scoring='accuracy', return_train_score=True)

"""#Winwebsec vs. Zbot"""

df1 = prep_df[prep_df['target'] <= 1]
x_train, x_test, y_train, y_test = model_selection.train_test_split(df1.drop(columns=['target']),df1['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("RFC Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#Winwebsec vs. ZeroAccess"""

df2 = prep_df[prep_df['target'].isin([0, 2])]
x_train, x_test, y_train, y_test = model_selection.train_test_split(df2.drop(columns=['target']),df2['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("RFC Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#Zbot vs. ZeroAccess"""

df3 = prep_df[prep_df['target'] > 0]
x_train, x_test, y_train, y_test = model_selection.train_test_split(df3.drop(columns=['target']),df3['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("RFC Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#Multi-Class"""

x_train, x_test, y_train, y_test = model_selection.train_test_split(prep_df.drop(columns=['target']),prep_df['target'],test_size=0.2, random_state=42)
gs.fit(x_train, y_train)
final_model = gs.best_estimator_
final_predictions = final_model.predict(x_test)
print(final_model.get_params())
print("RFC Accuracy Score -> ",accuracy_score(final_predictions, y_test)*100)

"""#Future (not currently working)"""

# tokenizer = BertTokenizer.from_pretrained('bert-base-cased', 
#                                           do_lower_case=False)
                                          
# encoded_data_train = tokenizer.batch_encode_plus(
#     df[df.data_type=='train'].tokenized_file.values, 
#     add_special_tokens=True, 
#     return_attention_mask=True, 
#     pad_to_max_length=True, 
#     max_length=256, 
#     return_tensors='np'
# )

# encoded_data_val = tokenizer.batch_encode_plus(
#     df[df.data_type=='test'].tokenized_file.values, 
#     add_special_tokens=True, 
#     return_attention_mask=True, 
#     pad_to_max_length=True, 
#     max_length=256, 
#     return_tensors='np'
# )

# input_ids_train = encoded_data_train['input_ids']
# attention_masks_train = encoded_data_train['attention_mask']
# labels_train = torch.tensor(df[df.data_type=='train'].target.values)

# input_ids_val = encoded_data_val['input_ids']
# attention_masks_val = encoded_data_val['attention_mask']
# labels_val = torch.tensor(df[df.data_type=='test'].target.values)

# model = BertForSequenceClassification.from_pretrained("bert-base-cased",
#                                                       num_labels=3,
#                                                       output_attentions=False,
#                                                       output_hidden_states=False)

# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# batch_size = 3
# dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)
# dataset_test = TensorDataset(input_ids_val, attention_masks_val, labels_val)
# dataloader_train = DataLoader(dataset_train, 
#                               sampler=RandomSampler(dataset_train), 
#                               batch_size=batch_size)

# dataloader_test = DataLoader(dataset_test, 
#                                    sampler=SequentialSampler(dataset_test), 
#                                    batch_size=batch_size)


# from sklearn.model_selection import train_test_split

# X_train, X_val, y_train, y_val = train_test_split(df.index.values, 
#                                                   df.label.values, 
#                                                   test_size=0.15, 
#                                                   random_state=42, 
#                                                   stratify=df.label.values)